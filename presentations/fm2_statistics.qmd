---
title: "FMII-SS25 Data analysis introduction"
subtitle: "How do I get my results?"
format:
  revealjs:
    mermaid:
      theme: default 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: img/uni_logo.jpg
    css: styles.css
    footer: '[all resources for you](https://saiko-psych.github.io/r-guide/)'
---

## **Goal of this session**

-   Learn how to **prepare** and **analyze** LimeSurvey data
-   Tools: **SPSS**, **Jamovi**, **R-Studio**, **PROCESS**
-   Understand key steps: preparation → analyzing → reporting

<br>

🧰 Hands-on and practical!

💬 Ask questions as we go!

::: notes
Speaker notes go here.
:::

## Why Prepare Your Data?

. . .

> “Garbage in, garbage out.” – Good data = good results.

. . .

-   Identify missing or incorrect data
-   Ensure correct variable types (nominal, ordinal, scale)
-   **Reproducibility**
-   Improved clarity
-   Easier analysis

. . .

> 90% of the time is data preperation, 10% is data analysis

::: footer

:::

::: notes
Speaker notes go here.as
:::

## Tools for Analysis

![](img/spss_logo.png){.absolute top="0" left="550" width="70" height="70"} ![](img/Jamovi_logo.png){.absolute top="0" left="650" width="70" height="70"} ![](img/jasp_logo.svg){.absolute top="0" left="750" width="70" height="70"} ![](img/RStudio.svg){.absolute top="0" left="850" width="70" height="70"}

<br>

::: table-small
| Tool | Strengths | Common Use Cases |
|------------------|------------------------|-------------------------------|
| SPSS | Robust, GUI-based, legacy support | Academic research, clinical trials, surveys |
| Jamovi | Easy UI, fast output, Open Source | Exploratory analyses, student projects |
| JASP | APA-style output, Open Source | Psychological research, Bayesian inference |
| R | Fully flexible, scriptable, Open Source | Reproducible science, data pipelines, automation |
:::

<br><span class="small-text"> and many, many more: Python, Julia, etc.

::: footer
find Setup guides [here]()
:::

::: notes
Speaker notes go here.
:::

## Access to SPSS

![](img/spss_logo.png){.absolute top="-20" left="450" width="70" height="70"}

1.  Use a PC at the University (e.g. PC-Room at the institut)

2.  Buy it (you can get it very cheap for one year while you study)

3.  Remote Desktop Access (very very slow ➜ use only in emergancy!)

::: footer
Software for Students [click here](https://it.uni-graz.at/de/it-services/arbeiten/software-fuer-studierende/)
:::

::: notes
Do not buy it! You can almost always get to a PC on Campus ➜ or better, use Open Source!
:::

## SPSS GUI - Screens

![](img/spss_logo.png){.absolute top="-20" left="550" width="70" height="70"}

::::: columns
::: {.column .small-text width="40%"}
-   **SPSS data files** have the extension `.sav`

    -   `Data View` & `Variable View` When you open SPSS you are in the `Data View`

-   **SPSS-Syntax** have the extension `.sps`

    -   programmable command language Automation, reproducibility

-   **SPSS-Outputs** have the extension `.spv`

    -   Result of the analyses & error messages
:::

::: {.column width="60%"}
![](img/spss_screen1.png) ![](img/spss_screen2.png){width="160" height="50"} <br>

![](img/spss_screen3.png)

<br> ![](img/spss_screen4.png)
:::
:::::

## SPSS **Syntax** specifics

![](img/spss_logo.png){.absolute top="-20" left="650" width="70" height="70"}

::: small-text
-   Commands can be selected and executed individually
-   Functions such as the creation/modification of variables, z-standardization/recoding of variables etc. are executed each time the syntax is run (if you run the whole syntax)!
:::

::: callout-tip
It is best to mark the syntax command as a comment afterwards!

Use (detailed) **Comments** („\*“ before your text) in the Syntax! There must be a period at the end of a command!

You can use AI-tools to create Syntax code ;) always use the `PASTE` function ➜ and save your syntax!
:::

## SPSS tips

![](img/spss_logo.png){.absolute top="-10" left="300" width="70" height="70"}

**ALWAYS use the “PASTE” command instead of “OK”** to document each step in the syntax.

-   you can edit and/or copy any command in the syntax
-   easy reproducable analysis

![](img/use_paste.png)

::: {.floating-box style="width: 150px; height: 75px; top: 400px; left: 170px;"}
:::

Use the “OK” button instead of “PASTE”, only if you are working with the PROCESS macro

::: notes
Even if you prefer the graphical user interface to working with the syntax.

also you can turn on in the options that you get the commands in the output also
:::


## Prerequisites for data analysis {.scrollable}

-   **Data Import** ️📥

    -   🗂️ Creating a suitable folder structure

    -   get raw data and save it propperly

    -   📂 Correct opening of the data set

    -   ✅ Verification of scales and labels

    -   🏷️ Meaningful naming of the variables

-   **Data Preperation** 🛠

    -   Create new Variables

    -   Recode existing Variables

    -   Define relevant variables

    -   filter relevant cases

::: notes
Speaker notes go here.
:::

## Good practice {.scrollable}

-   **Save raw data twice** and preferably once externally (backup copy)

-   **Create a separate “data work file”** (if something is accidentally deleted, the original raw data is still available!)

-   In Excel or similar: **document the number of excluded cases for each step** Incompletedata records, extreme values (filter), occasional missing values

-   **Always create new variables** (do not replace originals!)

-   Choose a clear and simple description of your data/syntax add missing labels for every Variable!!!

-   Select unique, simple and **meaningfull variable names**

## Data Import flowchart

```{mermaid}

flowchart TD
  subgraph s1[" "]
    A["📁 Create folder structure"]
    n3{"choose analysis tool"}
    n9["📥 Download csv file from LimeSurvey
    --> 💾 Save the raw file"]
    n10["📥 Download R(data file) & R(syntax file) & csv file from LimeSurvey
    --> 💾 Save the raw file"]
    n11["📥 Download SPSS-specific export & csv file from LimeSurvey
    --> 💾 Save the raw file"]
    n12["📝 name all variables and label them"]
    n13["🧪 Run the syntax file for the data file in R-Studio 
    -> labels and names the variables for you"]
    n14["💾 Save the prepared file"]
    n18["🧪 Run the syntax file for the data file in SPSS 
    -> labels and names the variables for you"]
    n19["📝 check if scales are correct"]
    n20["📝 check if scales, variable names and labels are correct 
    -> adjust if needed"]
    n21["📝 check if scales, variable names and labels are correct 
    -> adjust if needed"]
  end

  A --> n3
  n3 -- jamovi --> n9
  n3 -- "R-Studio" --> n10
  n3 -- SPSS --> n11
  n9 --> n12 --> n19 -->n14
  n10 --> n13 --> n20 --> n14
  n11 --> n18 --> n21 --> n14


  classDef Aqua stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
  classDef Peach stroke:#FBB35A, fill:#FFEFDB, color:#8F632D
  classDef Ash stroke:#999999, fill:#EEEEEE, color:#000000
  classDef Sky stroke:#374D7C, fill:#E2EBFF, color:#374D7C
  classDef Rose stroke:#FF5978, fill:#FFDFE5, color:#8E2236

  class n3 Aqua
  class n9 Peach
  class n12 Peach
  class n19 Peach
  class n11 Rose
  class n10 Sky
  class n13 Sky
  class n20 Sky
  class n18 Rose
  class n14 Ash

```

::: notes
Speaker notes go here.
:::

## Quick Overview in Scales

-   **Nominal**: Categories with no order.
-   **Ordinal**: Categories with a defined order.
-   **Interval**: Equal distances but no true zero.
-   **Ratio**: Equal distances and a true zero.

<br> Properly understanding the type of data scale helps with choosing the right **statistical methods**.

## The right scaling {.scrollable}

<br> **Nominal Scale**

-   **Categories with no order**.
-   Examples: Gender, Country, Eye Color.

**Ordinal Scale**

-   **Categories with a defined order**.
-   Differences between categories are **not uniform**.
-   Examples: Ranking (1st, 2nd, 3rd), Likert scale (Agree, Neutral, Disagree).

**Interval Scale**

-   **Ordered data with equal intervals**, but no absolute zero.
-   Examples: Temperature (Celsius, Fahrenheit), IQ Scores.

**Ratio Scale**

-   **Ordered data with equal intervals** and an **absolute zero**.
-   Examples: Height, Weight, Age, Income.

## Assigning Scales in your Software

<br>

**every variable is automatically assigned to a scale** which the software think is fitting **based on the values of the variable**

certain analysis need variables with certain scales

-   manually rescale needed varibales

![](img/scale_types.png){.absolute left="450"}

::: notes
In the analysis softwaresyou can only choose between Nominal, Ordinal or Continuuous ➜ you have to choose right ➜ most alnalysis only let you do the analysis if the variables have the right scale!

in all software tools **every variable is automatically assigned to a scale** which the software think is fitting **based on the values** ➜ many false scales (especially if you choose poor values in your survey)

but certail analysis need variables with certain scales!
:::

## Creating new Variables {.scrollable}

:::: panel-tabset
### info

#### Is needed for e.g.:

-   total scores (questionnaires)
-   Means (questionnaires, reaction times)
-   difference value (reaction times)

### GUI

![](img/new_variables_spss2.png){.absolute top="200" left="30" width="300"} ![](img/new_variables_spss1.png){.absolute top="200" left="400" width="600"}

::: {.floating-box style="width: 120px; height: 20px; top: 235px; left: 27px;"}
:::

### SPSS Syntax

``` spss
* create a total score
Compute random_sum=random1 + random2.
EXECUTE.

* create a mean score
Compute random_mean=(random1 + random2)/2.
EXECUTE.

* create a difference score
Compute random_diff=random1 - random2.
EXECUTE.
```

### SPSS tip

In the case of (e.g.) sum scores from a large number of variables, it is not necessary to enumerate all variables individually. The “`to`” command is used for this:

Instead:

``` spss

x_sum=var1 + var2 + …

-➜ 

x_sum=var1 to varN

```

In this case, however, the variables `var1` to `varN` must follow each other directly in the variable view!
::::

## Recoding existing variabels {.scrollable}

:::: panel-tabset
### info

-   remember to **ALWAYS create new variables**, never change existing ones

#### Is needed for e.g.:

-   Simplification of categorys
-   Inversion of scales
-   Creation of dummy variables
-   Error corrections
-   Mediansplit

### GUI

![](img/new_variables_spss2.png){.absolute top="200" left="30" width="300"} ![](img/recode_variable_spss1.png){.absolute top="200" left="380" width="400"} ![](img/recode_variable_spss2.png){.absolute top="400" left="700" width="400"}

::: {.floating-box style="width: 190px; height: 20px; top: 353px; left: 27px;"}
:::

### SPSS Syntax

``` spss

* simplification of categorys
RECODE alte_variable (1 THRU 3 = 1) (4 THRU 7 = 2) (8 THRU 10 = 3) INTO neue_variable.
EXECUTE.

* inversion of scales
RECODE alte_variable (1 = 6) (2 = 5) (3 = 4) (4 = 3) (5 = 2) (6 = 1) INTO neue_variable.
EXECUTE.

```

### SPSS tip

The rules for recoding can also be entered via the graphical user interface instead of the syntax! Click on *Change* to add the newly defined variable as an output variable
::::

## Descriptive statistics {.scrollable}

<br> 

### Characteristic values for describing data

  - Composition of the sample
  - “Basic information” about the variables used
  - (Verification of assumptions)

<br> 

### Important to understand your own data!!!

<br> 

### Different possibilities: 

  - (cumulative) absolute/relative frequencies
  - Measures of central tendency (modal value, median, mean value)
  - Dispersion measures (variance, standard deviation)
  - Graphical forms of presentation
  - Correlation


## Descriptive statistics SPSS - Frequencies {.scrollable}

:::::::::::: panel-tabset

### GUI

::: callout-note
Depending on the variable (scale level, number of gradations, ...), decide whether more or fewer measures make sense
:::


![](img/freq1.png){.absolute top="500" left="00" width="300"}

![](img/freq2.png){.absolute top="500" left="300" width="400"}

![](img/freq3.png){.absolute top="500" left="700" width="400"}
![](img/freq4.png){.absolute top="800" left="700" width="250"}

### Syntax

Exemplary syntax command

``` SPSS

FREQUENCIES VARIABLES=rand0 ast_erp
  /NTILES=4
  /STATISTICS=STDDEV VARIANCE RANGE MINIMUM MAXIMUM SEMEAN MEAN MEDIAN MODE SKEWNESS SESKEW 
    KURTOSIS SEKURT
  /HISTOGRAM NORMAL
  /ORDER=ANALYSIS.
  
```

### Output

![](img/freq5.png){.absolute top="260" left="00" width="400"}
![](img/freq6.png){.absolute top="400" left="430" width="500"}
![](img/freq7.png){.absolute top="600" left="400" width="500"}


::::::::::::


## Descriptive statistics SPSS - Descriptives + z-scores {.scrollable}

::: panel-tabset

### GUI

![](img/desc1.png){.absolute top="300" left="00" width="300"}

![](img/desc2.png){.absolute top="300" left="400" width="500"}
![](img/desc3.png){.absolute top="600" left="400" width="300"}

### Syntax


Exemplary syntax command

``` SPSS

DESCRIPTIVES VARIABLES=rand0 ast_erp
  /SAVE
  /STATISTICS=MEAN STDDEV VARIANCE RANGE MIN MAX SEMEAN KURTOSIS SKEWNESS.


* /SAVE is used to create your z-standardized variables


```



### Output 

Compared to “Frequencies”:

  - Same information (but fewer selection options) with a different arrangement

![](img/desc4.png){.absolute top="480" left="00" width="1000"}

### new z-scores

z-standardized variables appear in the data set and can be used for analyses

![](img/zscore1.png){.absolute top="400" left="00" width="450"}
![](img/zscore2.png){.absolute top="710" left="470" width="450"}


:::

## z-Standardization {.scrollable}

  - Standardization is usually used to compare variables with different units by transforming the measured values so that they have a given mean and variance

  - In z-standardization, the mean value of the variable $\bar{x}$  is subtracted from each measured value $x_i$  and the resulting difference is divided by the standard deviation $s_{emp}$  :

$$
z_i = \frac{x_i - \bar{x}}{s_{\text{emp}}}
$$


  - If this is carried out for all measured values of a variable, the result for the z-standardized measured values is a mean value of 0 and a standard deviation of 1


::: callout-note

z-standardization does not change the direction of the deviations of the individual measured values from the respective mean value!
:::


  - **Interpretation:** A z-value indicates by how many standard deviations a value deviates from the mean value


## Outliers in the data {.scrollable}

  - Are noticeably higher/lower than the majority of values and do not appear to fit with the remaining data points.
  - Often have a disproportionate influence on statistical analyses ➜ leading to skewed results.
  
<br>

### Identification

  - Visual: Scatterplots, Boxplots 
  - Statistical: z-values & 1.5 × IQR-rule 

<br>


### How to handle outliers

  - Consider possible causes of extreme values. <span class="small-note">
  (real values vs. Data entry errors)</span>
  - Set a threshold: Exclude certain values. <span class="small-note">(e.g. ±2.5/3 SD assuming a normal distribution)</span>
  - Data transformation (Logarithmize) to reduce the influence
  - Robust Methods <span class="small-note">(Median insted of Mittelwert, IQA instead of SD, robust Regression/Estimations, non-parametric Tests)</span>
  - (Bootstrapping)
  
  
>The approach should always be well thought out, logically justified, and documented.
Comparing the results with outliers and without outliers can also be insightful!



## Filter data in SPSS {.scrollable}

  - Reduction of the dataset to relevant cases for analysis (e.g., exclusion of outliers)
  - Targeted analysis of subgroups
  - SPSS creates a corresponding filter variable in the dataset!
  

#### It is possible to implement multiple Filters at once


  - Important at the end: "Turn off" the filter.
    - Either through the graphical user interface or via syntax.
    - Otherwise, the filter remains active, which can lead to errors in further analyses

::: panel-tabset

### GUI

![](img/filter1.png){.absolute top="850" left="00" width="300"}

![](img/filter2.png){.absolute top="850" left="350" width="650"}



### Syntax

Exemplary syntax command (easy)

``` SPSS

USE ALL.
COMPUTE filter_$=(lastpage=17 & alter>=18).
FILTER BY filter_$.
EXECUTE.

```
<br>

Exemplary syntax command (more complex ➜  via „PASTE“ )
➜  Enhances the documentation and user-friendliness of the syntax code!


``` SPSS

USE ALL.
COMPUTE filter_$=(lastpage = 17 & alter >= 18 ).
VARIABLE LABELS filter_$ 'lastpage = 17 & alter >= 18 (FILTER)'.
VALUE LABELS filter_$ 0 'Not Selected' 1 'Selected'.
FORMATS filter_$ (f1.0).
FILTER BY filter_$.
EXECUTE.


```

<br> 

#### Syntax command to deactivate Filters:


``` SPSS

Filter OFF.
USE ALL.
EXECUTE.


```

:::

## Filtering Outliers {.scrollable}

::: panel-tabset

### Example

  - Only individuals whose average reaction time deviates by a maximum of 2.5 standard deviations from the overall sample's average reaction time should be analyzed.

### Procedure

 - Standardize the variable with the average reaction time using the z-score (via „Descriptives“, see slide „Descriptive Statistics – Descriptives“)
 - Define a filter so that individuals with values < -2.5 and > 2.5 in the newly z-standardized variable are excluded.
 
![](img/filter4.png){.absolute top="550" left="30" width="800"}



### Syntax

``` SPSS

USE ALL.
COMPUTE filter_$=(ZSco01 > -2.5  &  ZSco01 < 2.5).
VARIABLE LABELS filter_$ 'ZSco01 > -2.5  &  ZSco01 < 2.5 (FILTER)'.
VALUE LABELS filter_$ 0 'Not Selected' 1 'Selected'.
FORMATS filter_$ (f1.0).
FILTER BY filter_$.
EXECUTE.


```

:::

## Check for normal distribution {.scrollable}

Theoretically, there are three possibilities:

  - **Inferential statistical testing**: Kolmogorov-Smirnov test or Shapiro-Wilk test
  - **Descriptive statistical testing**: Using skewness and kurtosis
  - **Graphical testing**: For example, using a histogram or Q-Q plot

<br>

➜ via „Explorative data analysis“ 

<br>

::: {.small-text}
::: callout-note
In some areas, skewness and kurtosis are preferred for assessing the normality of a variable (due to the disadvantages of inferential statistical methods). However, inferential statistical methods can generally still be used.

**References to check:**

- Mishra, P., Pandey, C. M., Singh, U., Gupta, A., Sahu, C., & Keshri, A. (2019). Descriptive statistics and normality tests for statistical data. Annals of cardiac anaesthesia, 22(1), 67–72. [https://doi.org/10.4103/aca.ACA_157_18](https://doi.org/10.4103/aca.ACA_157_18)
- Kim H. Y. (2013). Statistical notes for clinical researchers: assessing normal distribution (2) using skewness and kurtosis. Restorative dentistry & endodontics, 38(1), 52–54. [https://doi.org/10.5395/rde.2013.38.1.52](https://doi.org/10.5395/rde.2013.38.1.52)

::: 
:::


## Check for normal distribution SPSS {.scrollable}

:::::::::::: panel-tabset

### GUI-Input

![](img/normal1.png){.absolute top="200" left="30" width="300"}
![](img/normal2.png){.absolute top="200" left="350" width="300"}
![](img/normal3.png){.absolute top="200" left="650" width="200"}
![](img/normal4.png){.absolute top="370" left="650" width="200"}

### GUI output

#### Testing for normality using skewness and kurtosis

  - If both are less than 1 in absolute value, we can assume the variable is normally distributed
  - If the skewness is less than 1 in absolute value and the kurtosis is less than 5 in absolute value, we can still assume an acceptable fit
  
  
<br><br><br><br><br><br><br><br><br><br><br><br>

#### Histogram

<br><br><br><br><br><br><br><br><br><br><br>

#### Boxplot



::: small-text

The exploratory data analysis additionally provides boxplots, where potential outliers are directly marked (see slides on boxplots). However, the decision on what to do with outliers should again depend on substantive considerations and the expected impact of the outliers on the planned analysis. There is also the option to output extreme values of the variable, which can help gain a better understanding of the data

:::

<br><br><br><br><br><br><br><br><br><br><br>

#### Q-Q Plot

::: small-text
**In a "perfect" normal distribution, all data points in both graphs would lie on the black line.**

Q-Q plots provide a graphical illustration that can help better understand the data (especially the left plot). However, they should not be used as the sole criterion for deciding whether a variable is normally distributed.
:::


![](img/normal5.png){.absolute top="600" left="30" width="700"}

![](img/normal6.png){.absolute top="1300" left="30" width="750"}

![](img/normal7.png){.absolute top="2200" left="30" width="500"}


![](img/normal8.png){.absolute top="2400" left="600" width="350"}

![](img/normal9.png){.absolute top="2200" left="600" width="200"}


![](img/normal10.png){.absolute top="3000" left="0" width="500"}

![](img/normal11.png){.absolute top="3000" left="500" width="500"}


### Statistical Tests


Both the **Kolmogorov-Smirnov** test and the **Shapiro-Wilk-test** test the null hypothesis that the measurements of the respective variable are normally distributed

<br>

**A significant result indicates that the data are not normally distributed**

<br><br><br><br><br><br>

**Problem** with these tests:

  - Too high power in very large samples → significant results even for very small and practically insignificant deviations from "perfect normality“
  - Too low power in small samples → "problematic" deviations from normality are not detected, leading to no significant result


![](img/normal12.png){.absolute top="600" left="100" width="800"}

::::::::::::


## No normal distribution?

<br>

What to do if one or more variables to be analyzed are not normally distributed?

  - Transform the data (e.g., logarithmize)
  - Use the resampling method of bootstrapping
  - Argue using the central limit theorem
  - (Use non-parametric methods)


> Be transparent and report it!





## Reliability Analysis {.scrollable}

::: small-text
To assess the reliability (measurement accuracy, dependability, stability) of tests and measurements

Reliability values can be considered as the proportion of the true variance to the total variance of the test.

<br>

### Cronbachs Alpha (α)

<br>

Dependent on:

-   

    1.  number of items

-   

    2.  variance of the items

-   

    3.  variance of the test scores

-   

    4.  Covariance/intercorrelation of the items

α = Sample-dependent \| the more items the higher α

It is a generalization of the split-half method (where each item is considered as an independent test part)

<br>

#### A high α Is desirable for homogeneous constructs/tests

-   .7 (Heterogeneous constructs, personality tests)

-   .8 - .9 (Homogeneous constructs)

-   over .9 (Performance tests)

**CAUTION:** The frequently cited Kuder-Richardson formula (KR20) for dichotomous items is not explicitly provided in SPSS, but an adequate approximation can be calculated using Cronbach's alpha.

<br>

### Alternative: McDonalds Omega (ω)

Less strict assumptions (identical unstandardized loadings and error variances; essential τ-equivalence)

Recommended when the items correlate with the latent construct to varying degrees. Integrated since version 27 in SPSS! ➜ you can use it :)
:::

## Reliability Analysis {.scrollable}

:::::::::::: panel-tabset
### SPSS GUI

![](img/reliability_spss1.png){.absolute top="200" left="30" width="200"} ![](img/reliability_spss2.png){.absolute top="513" left="230" width="200"}

![](img/reliability_spss3.png){.absolute top="200" left="430" width="400"} ![](img/reliability_spss4.png){.absolute top="200" left="830" width="200"}

::: {.floating-box style="width: 190px; height: 20px; top: 517px; left: 27px;"}
:::

::: {.floating-box style="width: 190px; height: 20px; top: 530px; left: 227px;"}
:::

::: {.floating-box style="width: 60px; height: 13px; top: 500px; left: 460px;"}
:::

::: {.floating-box style="width: 190px; height: 100px; top: 230px; left: 825px;"}
:::

### SPSS Syntax

Exemplary syntax command

``` spss

RELIABILITY
  /VARIABLES=TSC1 TSC2r TSC3r TSC4r TSC5r TSC6 TSC7r TSC8 TSC9r TSC10r TSC11 TSC12r TSC13r
  /SCALE('ALL VARIABLES') ALL
  /MODEL=ALPHA
  /STATISTICS=DESCRIPTIVE SCALE CORR COV
  /SUMMARY=TOTAL CORR.

```

### SPSS tip

Items of a scale are dragged into the right field (be careful with reverse-coded items → use them accordingly!)

Click on the framed statistics

Copy the command into the syntax and execute it

### SPSS output

![](img/reliability_spss6.png){.absolute top="500" left="230" width="800"}

![](img/reliability_spss5.png){.absolute top="200" left="30" width="300"} ![](img/reliability_spss7.png){.absolute top="700" left="30" width="600"}

::: {.floating-box style="width: 110px; height: 89px; top: 450px; left: 27px;"}
:::

::: {.textbox style="top: 200px; left: 340px;"}
**Cronbachs Alpha Based on Standardized items** uses correlations instead of covariances. This can be used, for example, when the items of a test are measured in different units.

**Inter-Item Correlations**

The mean of the correlations between all item pairs within the measurement instrument. Very high inter-item correlations may indicate redundant items. .15 - .50 is acceptable
:::

::: {.floating-box style="width: 80px; height: 60px; top: 730px; left: 335px;"}
:::

::: {.floating-box style="width: 80px; height: 60px; top: 730px; left: 535px;"}
:::

::: {.textbox style="top: 750px; left: 640px;"}
**Corrected Item-Total Correlation**

= Discriminative power (correlation of an item with the scale, excluding the respective item) Items of a scale should be able – when there is a high reliability – to distinguish between individuals with low and high levels of the trait ➜ requires discriminative items. **\>.25 is acceptable; .40 - .70 is good**

<br>

**Cronbachs alpha if Item deletet**

= Improvement/deterioration of Cronbach's α by removing the respective item from the scale. In the present case, removing individual items would not lead to an improvement in α.
:::
::::::::::::


## t-test concept {.scrollable}


::::::: columns
:::: {.column width="60%"}
::: small-text

<br>

>checks whether the means of two groups differ significantly from each other or whether a mean differs significantly from a given value

<br>

### One-sample t-test

  - Does the mean of a sample differ significantly from a known or hypothetical population mean?

### Two-sample t-test for independent samples

  - Does the difference in means between two independent samples significantly deviate from a known or hypothetical population value?
  - "The special case is the normal case": Do two populations differ in their mean?

### Two-sample t-test for dependent samples

  - As with the two-sample t-test for independent samples, we are interested in the parameter differences here, but from dependent samples.

::: callout-tip
In all cases, the t-value quantifies how far the observed mean/the observed difference is from the expected value (under the null hypothesis) (numerator), measured in units of the standard error (denominator).

:::

:::

::::

:::: {.column width="40%"}

::: small-text

<br><br><br><br><br><br><br>

$$
t = \frac{\bar{x} - \mu_0}{\sqrt{\frac{s^2}{n}}}
$$
<br><br>
$$
t = \frac{(\bar{x}_1 - \bar{x}_2) - \mu_0}{\sqrt{\frac{s^2_{\text{pool}}}{n_1} + \frac{s^2_{\text{pool}}}{n_2}}}
$$

<br><br>
$$
t = \frac{(\bar{x}_1 - \bar{x}_2) - \mu_0}{\sqrt{\frac{s^2_{\text{diff}}}{n}}}
$$
:::
::::
:::::::


## t-test Assumptions {.scrollable}

<br>

- Interval-scaled dependent variable
- Normal distributed dependent variable (Check: see slides „Check for normal distribution“)

 - For two-sample t-test for independent samples: Homogeneity of variance
    - = Assumption that the variance of the dependent variable is equal across all groups of the factor
    - Checked using Levene's test (H0: estimated population variances are equal)
    - For a non-significant result, the values under "Equal variances assumed" are used for the t-test
    - For a significant result, the values under "Equal variances not assumed" are used for the t-test

**In principle, t-tests are quite robust to violations of assumptions (meaning they still provide stable results even when assumptions are violated).**



## t-test in SPSS{.scrollable}

:::::::::::: panel-tabset

### GUI

#### independent t-test
<br><br><br><br><br><br><br><br>
![](img/t-test1.png){.absolute top="300" left="30" width="400"} ![](img/t-test2.png){.absolute top="300" left="500" width="500"}

#### dependent t-test

::: callout-note
We define which variables are considered as dependent measurements or measurement pairs.

Here: Motivation at t1 and at t2.

::: 

![](img/t-test1.png){.absolute top="1100" left="30" width="400"} ![](img/t-teste.png){.absolute top="1100" left="500" width="500"}

::: {.floating-box style="width: 190px; height: 18px; top: 432px; left: 27px;"}
:::

::: {.floating-box style="width: 190px; height: 17px; top: 474px; left: 220px;"}
:::

::: {.floating-box style="width: 190px; height: 18px; top: 1232px; left: 27px;"}
:::

::: {.floating-box style="width: 190px; height: 17px; top: 1314px; left: 220px;"}
:::

### SPSS Syntax

Exemplary syntax command

``` spss

* independent t-test

T-TEST GROUPS=rand0(0 1)
  /MISSING=ANALYSIS
  /VARIABLES=ast_erp
  /ES DISPLAY(TRUE)
  /CRITERIA=CI(.95).



* dependent t-test

T-TEST PAIRS=motiva1 WITH motiva2 (PAIRED)
  /ES DISPLAY(TRUE) STANDARDIZER(SD)
  /CRITERIA=CI(.9500)
  /MISSING=ANALYSIS


```

### SPSS tip

Items of a scale are dragged into the right field (be careful with reverse-coded items → use them accordingly!)

Click on the framed statistics

Copy the command into the syntax and execute it

### SPSS output



![](img/t-test4.png){.absolute top="200" left="0" width="1000"} 
![](img/t-test5.png){.absolute top="700" left="0" width="1000"}

::: {.floating-box style="width: 140px; height: 110px; top: 242px; left: 270px;"}
:::

::: {.textbox style="top: 400px; left: 540px;"}
Levene's test to check the assumption of homogeneity of variance

:::


::: {.textbox style="top:970px; left: 680px;"}
In the context of a t-test for dependent samples, the assumption of homogeneity of variance does not need to be checked. However, interval scaling and normality should still be assessed.

:::
::::::::::::

::: notes
Speaker notes go here.
:::

::: notes
Speaker notes go here.
:::

## ANOVA Assumptions

![](img/anova_assumptions.png){.absolute top="" left="" width="1000"}

## ANOVA - Effect Sizes $\hat{\eta}^2 \, \, \hat{\eta}^2_p$ {.scrollable}

::::::: columns
:::: {.column width="60%"}
::: small-text
→ Both are measures of effect size within the context of analysis of variance

**η2** : Proportion of the total variance explained by the factor

-   How much of the variance in the dependent variable can be attributed to the individual factor?
-   Contra: Comparison of effect size with other analyses is not possible, as the effect size depends on which other effects are being tested in the analysis.

<br> **η2p:** All components except the variance of interest and the error variance are removed from the total variance (in the denominator)

-   What is the proportion of the dependent variable's variance attributed to the individual factor and the error that can be explained by each respective factor?
-   Pro: Comparable across different studies (small)
-   Contra: Partial effect size measures within a study cannot be summed to a total effect → Representation of the relationships between the effects within the analysis is not possible
:::
::::

:::: {.column width="40%"}
::: small-text
<br><br>

$\hat{\eta}^2 = \frac{QS_{\text{Factor}}}{QS_{\text{Total}}}$

<br><br><br><br><br><br><br><br><br><br>

$\hat{\eta}^2_p = \frac{QS_{\text{Factor}}}{QS_{\text{Factor}} + QS_{\text{Error}}}$
:::
::::
:::::::



## One-Way ANOVA without repeated measurements SPSS {.scrollable}

Before we start with the analysis we should check if our Assumptions are met!

This ANOVA is used to examine differences in a variable across more than 2  (independent) groups

::: panel-tabset

### GUI

![](img/anova_within_in_1.png){.absolute top="500" left="" width="300"}

![](img/anova_within_in_2.png){.absolute top="500" left="340" width="300"}

![](img/anova_within_in_3.png){.absolute top="500" left="670" width="300"}

![](img/anova_within_in_4.png){.absolute top="850" left="670" width="300"}
![](img/anova_within_in_5.png){.absolute top="1060" left="670" width="300"}

![](img/anova_within_in_6.png){.absolute top="1300" left="670" width="300"}



### EMM

Also referred to as "estimated marginal means" or "least squares means“

These estimated means represent a form of “standardized means”

Important for interpreting the effects of categorical predictors within the model, especially when the sample sizes between the groups vary or when the model includes covariates

Always select for interactions and when an independent variable has more than 2 levels!

<br>

**Caution:** Post-Hoc-Tests under `Post Hoc... ` are generally only possible for between-subjects factors. For multifactorial ANOVAs, it is best to conduct these through "EM Means"

<br>

USE BONFERRONI

![](img/emm.png){.absolute top="1400" left="" width="1000"}


### Syntax

Exemplary syntax command

``` SPSS

UNIANOVA Pruefungsangst_T1 BY Studienrichtung
  /METHOD=SSTYPE(3)
  /INTERCEPT=INCLUDE
  /PLOT=PROFILE(Studienrichtung) TYPE=BAR ERRORBAR=CI MEANREFERENCE=NO
  /EMMEANS=TABLES(Studienrichtung) COMPARE ADJ(BONFERRONI)
  /PRINT ETASQ DESCRIPTIVE HOMOGENEITY
  /CRITERIA=ALPHA(.05)
  /DESIGN=Studienrichtung.


```


### Output

![](img/anova_within_out_1.png){.absolute top="500" left="" width="400"}
![](img/anova_within_out_2.png){.absolute top="500" left="500" width="500"}


![](img/anova_within_out_3.png){.absolute top="800" left="" width="1000"}

![](img/anova_within_out_4.png){.absolute top="1200" left="" width="1000"}
![](img/anova_within_out_5.png){.absolute top="1600" left="" width="800"}
![](img/anova_within_out_6.png){.absolute top="2000" left="" width="1000"}

![](img/anova_within_out_7.png){.absolute top="2500" left="" width="1000"}

:::


::: {.notes}


The Levene's test for homogeneity of variances checks whether the variances in the different groups are equal (= H0).If a significant result is found, it can be "ignored" in the case of a conservative method for correction in pairwise comparisons (e.g., Bonferroni), but it should still be reported!

:::


## ANOVA reporting results {.scrollable}


::: panel-tabset

### Info

Relevant descriptive statistics (either mentioned in the text or referenced in an APA-compliant table)

  - Factor + levels
  - N, M, SD of the groups
  - Estimated population means and standard errors for (used) pairwise comparisons
  
  
F-/Omnibus test: 

  - *F*(df_UV, df_Error) = F-value, *p* = p-value, $\eta_p^2$  = value


Mean difference, standard error, and p-value for all pairwise comparisons

  - Significant pairwise comparisons should be discussed in the text. Non-significant pairwise comparisons can be presented in an APA-compliant table, with a reference to the table in the text
  
In the case of a significant Mauchly's Test of Sphericity in a one-way ANOVA with a within-subjects factor, the correction method used (e.g., Greenhouse-Geisser) must also be reported.

### Example

Always prioritize the substantive conclusion, with statistical values serving only to support the statement!

  - Don‘t: "The three groups differ significantly from each other.“
  
#### Example Report

> „A one-way repeated measures ANOVA was conducted with the repeated measures factor [Factor Name]. The descriptive statistics can be found in Table 1. The results indicated that students from the fields of psychology, history, and business administration significantly differed in perceived exam anxiety (report F-test correctly). Pairwise comparisons further revealed that psychology students had significantly higher levels of exam anxiety than business administration students (report pairwise comparison). No significant differences were found otherwise (see Table X; reference to table with non-significant pairwise comparisons)”


:::


## PROCESS Macro

-   

    ### installation

-   

    ### use for moderation- & mediation analysis

## PROCESS installation {.scrollable}

<br>

**PROCESS** is a macro for SPSS developed by Andrew F. Hayes specifically designed for the analysis of mediated, moderated and conditional process models.

It simplifies the application of regression analyses, especially for more complex models that examine interaction effects or indirect mediation effects.

<br> **Integration in SPSS**

1.  <https://processmacro.org/download.html>

2.  <https://haskayne.ucalgary.ca/CCRAM/resource-hub>

```         
“Backup download link” should automatically download the desired .zip folder.

Alternatively, you can take the detour via the website of the Canadian Centre for Research Analysis and Methods (step 2)
```

3.  Unzip the .zip-folder & start „process.sps“-(Syntax-) file

4.  In the open SPSS-Syntax: `Extensions ➜ Utilities ➜ Install custom Dialog (Compatibility Mode)`

5.  Select the folder, in witch the file „process.spd“ is safed + open it

6.  You can select the macro under `Analyze ➜ Regression`

## Moderation concept {.scrollable}

![](img/mod_1.png){.absolute top="" left="" width=""} ![](img/mod_2.png){.absolute top="600" left="" width=""}

![](img/mod_3.png){.absolute top="1250" left="" width=""} <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

### Interpretation of (unstandardized) regression-weights if a moderation effect exists

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

## Moderation analysis with Process

![](img/process_mod_1.png){.absolute top="70" left="" width=""}

## PROCESS - Output {.scrollable}

::: panel-tabset
### info

![](img/process_mod_4.png){.absolute top="200" left="" width=""}

### Output

![](img/process_mod_2.png){.absolute top="200" left="" width=""} \###

### JND

![](img/process_mod_3.png){.absolute top="200" left="" width=""}

### PROCESS Interaction Diagram

![](img/process_mod_5.png){.absolute top="200" left="" width=""}
:::

## PROCESS Limitations {.scrollable}

::: small-text
In this example, a dichotomous IV and a metric moderator variable (and their interaction term) were included in the regression model.

-   If you use a dichotomous moderator, there is no need to output the Johnson & Neyman significance regions
-   The use of a categorical variable (as IV or moderator ➜ dummy coding) is not explicitly considered here. Instructions can be found on the Internet.

PROCESS CANNOT be used to Therefore, the interaction term IV \* moderator must first be created in SPSS in the form of a new variable

-   `Transform ➜ Compute variable`

-   This variable is then entered into the regression together with IV and moderator in the regression
:::

## Interpretation of Moderation Analysis {.scrollable}

![](img/process_mod_int1.png){.absolute top="70" left="" width=""} ![](img/process_mod_int2.png){.absolute top="570" left="" width=""}

## Estimation and interpretation of the parameters of centered predictors

![](img/process_mod_int3.png){.absolute top="170" left="" width=""}

## Example – simple Moderation

![](img/process_mod_int4.png){.absolute top="70" left="" width=""}

## Mediation concept

![](img/med_1.png){.absolute top="70" left="" width=""}

## Mediation PROCESS {.scrollable}

![](img/process_med_1.png){.absolute top="50" left="" width=""}

![](img/process_med_2.png){.absolute top="500" left="0" width=""} ![](img/process_med_3.png){.absolute top="950" left="" width=""}

## Mediation - presenting results

No general standards available

**Recomendations:** Graphical representation + reporting results in the text or APA-table for MLR Path coefficients (a, b, a\*b, c‘) ➜ (un)standardized, SE, bootstrapping confidence interval

<br>

**Effect size:** Dichotomous predictor → partially standardized effect (unstd. a*b / SDY) Continuous predictor → fully standardized effect (unstd. a*b / SDY / SDX )

Further guidelines for result presentation can be found online e.g. [http://www.regorz-statistik.de/inhalte](http://www.regorz-statistik.de/inhalte/tutorial_mediator_bootstrapping_process.html#:~:text=Beim%20partiell%20standardisierten%20Effekt%20wird,an%20der%20abh%C3%A4ngigen%20Variable%20standardisiert).

## Mediation with categorical predictors

A detailed description can be found, for example, in A. Field (2018). Please feel free to reach out by email if you have any questions regarding your analysis!

Key points to remember: A categorical variable is dummy-coded One group serves as the reference/baseline for all other groups → 0-coding

Video-tutorial: <https://www.youtube.com/watch?v=r2_zw4G55X0>


## Access to jamovi

![](img/Jamovi_logo.png){.absolute top="-20" left="490" width="70" height="70"}

1.  Use [jamovi Cloud](https://www.jamovi.org/cloud.html)

2.  Download jamovi [here](https://www.jamovi.org/download.html)

    -   choose right installation (e.g. Windows, Mac, linux)
    -   download and install

::: footer
help with [jamovi forum](https://forum.jamovi.org/)
:::

::: notes
Just get it, its easy to get and easy to use! perfect for quick analysis
:::

## Access to JASP

![](img/jasp_logo.svg){.absolute top="-14" left="450" width="70" height="70"}

1.  Use [JASP online](https://www.rollapp.com/launch/jasp)

2.  Download JASP [here](https://jasp-stats.org/download/)

    -   choose right installation (e.g. Windows, Mac, linux)
    -   download and install

::: footer
help with [JASP forum](https://forum.cogsci.nl/categories/jasp-bayesfactor)
:::

::: notes
i dont have much exerience with jasp but i think it is very good, and from uni amsterdam, which is the best methological psychology university in europe according to me
:::

## Access to R-Studio

![](img/RStudio.svg){.absolute top="-14" left="550" width="70" height="70"}

<br>

You need to install R first and the R-Studio!

1.  [Download R](https://cran.rstudio.com/)

2.  [Download R-Studio](https://posit.co/download/rstudio-desktop/)

    -   choose right installation (e.g. Windows, Mac, linux)
    -   download and install
    -   setup ➜ and only use R-Studio ➜ never R

::: footer
help with [R-Studio forum](https://forum.posit.co/)
:::

::: notes
The sooner you start with R-Studio the better for yourself ➜ it is such a powerfull tool
:::



## Reporting Your Results

-   Follow APA guidelines
-   Report:
    -   Test type
    -   Test statistic (e.g., *t*, *F*, *r*)
    -   p-values and effect sizes
    -   Graphs when useful (everything has to be somewhere ONCE)
    -   use the sheet online

✅ Example: \> "A significant pearson correlation was found between X and Y, *r* = .45, *p* \< .001"

::: notes
Speaker notes go here.
:::

## Resources & Support

-   [Saiko Psychology R Guide](https://saiko-psych.github.io/r-guide/)
-   [SPSS video tutorials](https://www.youtube.com/watch?v=b163iBByycw&list=PL25257A24840423AE)
-   [discovR](https://www.discovr.rocks/discovr/)
-   [Quarto tutorials](https://www.youtube.com/watch?v=4rKWIJXe3mA&list=PLEzw67WWDg80-fT1hq2IZf7D62tRmKy8f)
-   [R-Studio tutorials](https://www.youtube.com/watch?v=b163iBByycw&list=PL25257A24840423AE)
-   Ask your instructor or TA
-   AI Tools (e.g. Mistral, Claude)

🧠 Practice = confidence!

::: notes
Speaker notes go here.
:::

## Let's Practice!

-   Open your Data in SPSS
-   Identify variable types
-   Run a descriptive analysis
-   Calculate Cronbach's alpha
-   Try a correlation or t-test

💬 Ask questions as we go!

::: notes
Speaker notes go here.
:::
